{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "we use logistic regression for binary classification , for those outputs are discrete ( 0 , 1 ) and it belongs to two classes in total.Logistic Regression only outputs the probablity values that features may belong the given categories ( 0 ,1 ).\n",
    "## First order Optimization\n",
    "\n",
    "In first order optimization method we will use the jacovian matrix to update the *weights* and *bias* values.\n",
    "$$\n",
    "\\begin{align}\n",
    "w_\\theta &= \\mathcal{V} - w_\\theta^` \\qquad , \\mathcal{V} = \\space \\text{learning rate (0 to 1) } \\\\\n",
    "b_\\theta &= \\mathcal{V} - b_\\theta^` \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is a helper function to load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    df=pd.read_csv(url,header=None);\n",
    "    return ( df.iloc[:,:-1] , df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are using `Sigmoid funtion` for activation . which takes input from - $\\infty$ to $\\infty$ and outputs from $0$ to $1$ . which is very usefull to find probability values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    returns sigmoid h(x)= 1/(e^-x + 1) of the input x\n",
    "    '''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this `utility function` checks the gradient descent has converged , if converged we will stop the gradient descent training .\n",
    "So convergence is checked by finding the difference between the *old* $w_\\theta$ and *new* $w_\\theta$ if there is'nt any much of difference then its converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_convergence(beta_old,beta_new,tol=1e-3):\n",
    "    '''\n",
    "    Checks whether the coefficients have converged in the l-infinity norm.\n",
    "    Returns True if they have converged, False otherwise.'''\n",
    "    #calculate the change in the coefficients\n",
    "    coef_change = np.abs(beta_old - beta_new)\n",
    "    \n",
    "    #if change hasn't reached the threshold and we have more iterations to go, keep training\n",
    "    return not (np.any(coef_change>tol) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loss Function :</h3>\t\n",
    "\t\t\t\t\t\n",
    "$$\n",
    "Cost / Loss =\n",
    " \\begin{cases}\n",
    " -log(h_{\\theta}(x)) & \\text{if $y$ is 1} \\\\\n",
    " -log(1-h_{\\theta}(x)) & \\text{if $y$ is 0}\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "Which can be simplified as below ,\n",
    "\n",
    "$$\n",
    "loss = \\frac{1}{m}(-ylog(h_{\\theta}(x) \\ -(1-y)( \\ log(1-h_{\\theta}(x) \\ ) \\ ) \\ ) \n",
    "$$\n",
    "\n",
    "\n",
    " let's check this figure below and see how the Cost / Loss function works .\n",
    "\n",
    " when $y = 1$ , we plotting $-log(h(x))$\n",
    "\n",
    "![mtVFHjoeQkaNuKP](https://i.loli.net/2019/08/27/mtVFHjoeQkaNuKP.png)\n",
    "\n",
    "if $h(x)$ is close to $1$ , cost is so low wherelse when $h(x)$ is close to 0 , cost is so high \n",
    "\n",
    "When $y=0$ , we plotting $-log(1-h(x))$\n",
    "\n",
    "![EumjY89IzUcVh1S](https://i.loli.net/2019/08/27/EumjY89IzUcVh1S.png)\n",
    "\n",
    "if $h(x)$ is close to $0$ , cost is so low wherelse when $h(x)$ is close to $1$ , cost is so high , make sense now !\n",
    "\n",
    "The Deriavative of loss function is below ,\n",
    "\n",
    "$$ \\frac{\\partial loss}{\\partial w} = \\frac{(h_\\theta(x) - y)x^T}{m} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
