{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous lesson you learned about the different approaches for reducing the dimensionality of a dataset using different feature selection techniques. An alternative approach to feature selection for dimensionality reduction is **feature extraction**.\n",
    "\n",
    "In this lesson, you will learn about three fundamental techniques that will help us to summarize the information content of a dataset by transforming it onto a new feature subspace of lower dimensionality than the original one. Data compression is an important topic in machine learning, and it helps us to store and analyze the increasing amounts of data that are produced and collected in the modern age of technology.\n",
    "\n",
    "We will cover the following topics:\n",
    "\n",
    "- **Principal Component Analysis** (**PCA**) for unsupervised data compression\n",
    "- **Linear Discriminant Analysis** (**LDA**) as a supervised dimensionality reduction technique for maximizing class separability\n",
    "- Nonlinear dimensionality reduction via **Kernel Principal Component Analysis** (**KPCA**)\n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "Similar to feature selection, we can use different feature extraction techniques to reduce the number of features in a dataset. The difference between feature selection and feature extraction is that while we maintain the original features when we used feature selection algorithms, such as **sequential backward selection**, we use feature extraction to transform or project the data onto a new feature space.\n",
    "\n",
    "In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. In practice, feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the **curse of dimensionality**—especially if we are working with non-regularized models.\n",
    "\n",
    "### **The Main Steps Behind PCA**\n",
    "\n",
    "In this step, we will discuss PCA, an unsupervised linear transformation technique that is widely used across different fields, most prominently for feature extraction and dimensionality reduction. Other popular applications of PCA include exploratory data analyses and de-noising of signals in stock market trading, and the analysis of genome data and gene expression levels in the field of bioinformatics.\n",
    "\n",
    "PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other, as illustrated in the following figure\n",
    "\n",
    "![iamg](https://cdn.filestackcontent.com/URUouEblQ3Gq36aXAGcK)\n",
    "\n",
    "In the preceding figure, *x1* and *x2* are the original feature axes, and **PC1** and **PC2** are the principal components.\n",
    "\n",
    "If we use PCA for dimensionality reduction, we construct a d \\times k*d*×*k*–dimensional transformation matrix ***W*** that allows us to map a sample vector ***x*** onto a new *k*–dimensional feature subspace that has fewer dimensions than the original *d*–dimensional feature space:\n",
    "\n",
    "$$dx=[x  1 ​\t  ,x  2 ​\t  ,…,x  d ​ ],  x∈R  d  \\\\ ↓xW,  W∈R  d×k \\\\ z=[z  1 ​\t  ,z  2 ​\t  ,…,z  k ​\t  ],  z∈R  k$$\n",
    "\n",
    "As a result of transforming the original *d*-dimensional data onto this new *k*-dimensional subspace (typically *k* << *d*), the first principal component will have the largest possible variance, and all consequent principal components will have the largest variance given the constraint that these components are uncorrelated (orthogonal) to the other principal components—even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated).\n",
    "\n",
    "Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features *prior* to PCA if the features were measured on different scales and we want to assign equal importance to all features.\n",
    "\n",
    "Before looking at the PCA algorithm for dimensionality reduction in more detail, let's summarize the approach in a few simple steps:\n",
    "\n",
    "1. Standardize the *d*-dimensional dataset.\n",
    "2. Construct the covariance matrix.\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "5. Select *k* eigenvectors which correspond to the *k* largest eigenvalues, where *k* is the dimensionality of the new feature subspace (*k*≤*d*).\n",
    "\n",
    "    $$k \\le d$$\n",
    "\n",
    "6. Construct a projection matrix ***W*** from the \"top\" *k* eigenvectors.\n",
    "7. Transform the *d*-dimensional input dataset ***X*** using the projection matrix ***W*** to obtain the new *k*-dimensional feature subspace.\n",
    "\n",
    "Let’s perform a PCA step by step, using Python as a learning exercise. Then, we will see how to perform a PCA more conveniently using `scikit-learn`.\n",
    "\n",
    "### **Extracting the Principal Components Step By Step**\n",
    "\n",
    "In this subsection, we will tackle the first four steps of a PCA. Open the Jupyter notebook `dimensionality_reduction.ipynb` to get started!\n",
    "\n",
    "First, we will start by loading the *Wine* dataset that we worked with in *Lesson 1*, *Data Preprocessing*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from helper import plot_decision_regions\n",
    "from rbf_kernel_pca import rbf_kernel_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',header=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "5   1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97  6.75  1.05  2.85   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  \n",
       "5  1450  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y =  df_wine.iloc[:, 0].values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_wine.iloc[:, 1:].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will process the *Wine* data into separate training and test sets—using a 70:30 split—and standardize it to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y , test_size=0.3,\n",
    "    stratify=Y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the mandatory preprocessing, let's advance to the second step: constructing the covariance matrix. The symmetric $d \\times d$-dimensional covariance matrix, where $d$ is the number of dimensions in the dataset, stores the pairwise covariances between the different features. For example, the covariance between two features $x_j$ and $x_k$ on the population level can be calculated via the following equation:\n",
    "\n",
    "$$σ_{jk}​=\\frac{1}{n}\\sum_{i=1}^{n}​(x_j^{(i)}​−μ_j​)(x_k^{(i)}​−μ_k​)$$\n",
    "\n",
    "Here,  $\\mu_j$ and  $\\mu_k$are the sample means of features *j* and *k*, respectively.\n",
    "\n",
    "Note that the sample means are zero if we standardized the dataset. A positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates that the features vary in opposite directions. For example, the covariance matrix of three features can then be written as follows (note that $\\Sigma$ stands for the Greek uppercase letter **sigma**, which is not to be confused with the **sum** symbol):\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix} \\sigma_1^2 & \\sigma_{12} & \\sigma_{13} \\\\ \\sigma_{21} & \\sigma_2^2 & \\sigma_{23} \\\\ \\sigma_{31} & \\sigma_{32} & \\sigma_3^2 \\end{bmatrix}$$\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. In the case of the *Wine* dataset, we would obtain 13 eigenvectors and eigenvalues from the 13 x 13-dimensional covariance matrix.\n",
    "\n",
    "Now, for our third step, let's obtain the eigenpairs of the covariance matrix. As we remember from our introductory linear algebra classes, an eigenvector ***v*** satisfies the following condition:\n",
    "\n",
    "$$Σv=λv$$\n",
    "\n",
    "Here, $\\lambda$ is a scalar: the eigenvalue. Since the manual computation of eigenvectors and eigenvalues is a somewhat tedious and elaborate task, we will use the `linalg.eig` function from `NumPy` to obtain the eigenpairs of the *Wine* covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Values \n",
      " [4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634\n",
      " 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835\n",
      " 0.1808613 ] \n",
      "\n",
      "Eigen Vectors \n",
      " [[-1.37242175e-01  5.03034778e-01 -1.37748734e-01 -3.29610003e-03\n",
      "  -2.90625226e-01  2.99096847e-01  7.90529293e-02 -3.68176414e-01\n",
      "  -3.98377017e-01 -9.44869777e-02  3.74638877e-01 -1.27834515e-01\n",
      "   2.62834263e-01]\n",
      " [ 2.47243265e-01  1.64871190e-01  9.61503863e-02  5.62646692e-01\n",
      "   8.95378697e-02  6.27036396e-01 -2.74002014e-01 -1.25775752e-02\n",
      "   1.10458230e-01  2.63652406e-02 -1.37405597e-01  8.06401578e-02\n",
      "  -2.66769211e-01]\n",
      " [-2.54515927e-02  2.44564761e-01  6.77775667e-01 -1.08977111e-01\n",
      "  -1.60834991e-01  3.89128239e-04  1.32328045e-01  1.77578177e-01\n",
      "   3.82496856e-01  1.42747511e-01  4.61583035e-01  1.67924873e-02\n",
      "  -1.15542548e-01]\n",
      " [ 2.06945084e-01 -1.13529045e-01  6.25040550e-01  3.38187002e-02\n",
      "   5.15873402e-02 -4.05836452e-02  2.23999097e-01 -4.40592110e-01\n",
      "  -2.43373853e-01 -1.30485780e-01 -4.18953989e-01 -1.10845657e-01\n",
      "   1.99483410e-01]\n",
      " [-1.54365821e-01  2.89745182e-01  1.96135481e-01 -3.67511070e-01\n",
      "   6.76487073e-01  6.57772614e-02 -4.05268966e-01  1.16617503e-01\n",
      "  -2.58982359e-01 -6.76080782e-02  1.00470630e-02  7.93879562e-02\n",
      "   2.89018810e-02]\n",
      " [-3.93769523e-01  5.08010391e-02  1.40310572e-01  2.40245127e-01\n",
      "  -1.18511144e-01 -5.89776247e-02 -3.47419412e-02  3.50192127e-01\n",
      "  -3.42312860e-01  4.59917661e-01 -2.21254241e-01 -4.91459313e-01\n",
      "  -6.63868598e-02]\n",
      " [-4.17351064e-01 -2.28733792e-02  1.17053859e-01  1.87053299e-01\n",
      "  -1.07100349e-01 -3.01103180e-02  4.17835724e-02  2.18718183e-01\n",
      "  -3.61231642e-02 -8.14583947e-01 -4.17513600e-02 -5.03074004e-02\n",
      "  -2.13349079e-01]\n",
      " [ 3.05728961e-01  9.04888470e-02  1.31217777e-01 -2.29262234e-02\n",
      "  -5.07581610e-01 -2.71728086e-01 -6.31145686e-01  1.97129425e-01\n",
      "  -1.71436883e-01 -9.57480885e-02 -8.87569452e-02  1.75328030e-01\n",
      "   1.86391279e-01]\n",
      " [-3.06683469e-01  8.35232677e-03  3.04309008e-02  4.96262330e-01\n",
      "   2.01634619e-01 -4.39997519e-01 -3.23122775e-01 -4.33055871e-01\n",
      "   2.44370210e-01  6.72468934e-02  1.99921861e-01 -3.67595797e-03\n",
      "   1.68082985e-01]\n",
      " [ 7.55406578e-02  5.49775805e-01 -7.99299713e-02  1.06482939e-01\n",
      "   5.73607091e-03 -4.11743459e-01  2.69082623e-01 -6.68411823e-02\n",
      "  -1.55514919e-01  8.73336218e-02 -2.21668868e-01  3.59756535e-01\n",
      "  -4.66369031e-01]\n",
      " [-3.26132628e-01 -2.07164328e-01  5.30591506e-02 -3.69053747e-01\n",
      "  -2.76914216e-01  1.41673377e-01 -3.02640661e-01 -4.59762295e-01\n",
      "   2.11961247e-02  1.29061125e-01 -9.84694573e-02  4.04669797e-02\n",
      "  -5.32483880e-01]\n",
      " [-3.68610222e-01 -2.49025357e-01  1.32391030e-01  1.42016088e-01\n",
      "  -6.66275572e-02  1.75842384e-01  1.30540143e-01  1.10827548e-01\n",
      "  -2.38089559e-01  1.87646268e-01  1.91205783e-02  7.42229543e-01\n",
      "   2.37835283e-01]\n",
      " [-2.96696514e-01  3.80229423e-01 -7.06502178e-02 -1.67682173e-01\n",
      "  -1.28029045e-01  1.38018388e-01  8.11335043e-04  5.60817288e-03\n",
      "   5.17278463e-01  1.21112574e-02 -5.42532072e-01  3.87395209e-02\n",
      "   3.67763359e-01]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cov_matrix = np.cov(X_train_std.T)\n",
    "eig_values , eig_vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print('Eigen Values \\n',eig_values,'\\n')\n",
    "print('Eigen Vectors \\n',eig_vectors,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `numpy.cov` function, we computed the covariance matrix of the standardized training dataset. Using the `linalg.eig` function, we performed the eigen decomposition, which yielded a vector (`eigen_vals`) consisting of 13 eigenvalues and the corresponding eigenvectors stored as columns in a 13 x 13-dimensional matrix (`eigen_vecs`).\n",
    "\n",
    "### **Total and Explained Variance**\n",
    "\n",
    "Since we want to reduce the dimensionality of our dataset by compressing it onto a new feature subspace, we only select the subset of the eigenvectors (principal components) that contains most of the information (variance). The eigenvalues define the magnitude of the eigenvectors, so we have to sort the eigenvalues by decreasing magnitude; we are interested in the top *k* eigenvectors based on the values of their corresponding eigenvalues.\n",
    "\n",
    "But before we collect those *k* most informative eigenvectors, let us plot the **variance explained ratios** of the eigenvalues. The variance explained ratio of an eigenvalue $\\lambda_j$​ is simply the fraction of an eigenvalue $\\lambda_j$​ and the total sum of the eigenvalues:\n",
    "\n",
    "$$\\frac{\\lambda_j}{\\sum_{j=1}^d \\lambda_j}$$\n",
    "\n",
    "Using the `NumPy` `cumsum` function, we can then calculate the cumulative sum of explained variances, which we will then plot via `matplotlib`'s `step` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted in Descending Order [4.842745315655898, 2.4160245870352255, 1.5484582488203513, 0.9612043774977367, 0.8416616104578422, 0.6620634040383039, 0.5182847213561953, 0.34650376641286657, 0.3131368004720887, 0.2135721466052733, 0.18086130479496634, 0.15362835006711043, 0.10754642369670996]\n"
     ]
    }
   ],
   "source": [
    "total = sum(eig_values)\n",
    "sorted_evalues = sorted(eig_values,reverse=True)\n",
    "print('sorted in Descending Order', sorted_evalues)\n",
    "var_exp = [i/total for i in sorted_evalues]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no  13 13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('  ',len(var_exp) , len(cum_var_exp) )\n",
    "# plot explained variances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(1,len(var_exp)+1), var_exp, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot indicates that the first principal component alone accounts for approximately 40% of the variance. Also, we can see that the first two principal components combined explain almost 60% of the variance in the dataset.\n",
    "\n",
    "Although the explained variance plot reminds us of the feature importance values that we computed in *Lesson 1*, *Data Preprocessing*, via random forests, we should remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored. Whereas a random forest uses the class membership information to compute the node impurities, variance measures the spread of values along a feature axis.\n",
    "\n",
    "### **Feature Transformation**\n",
    "\n",
    "After we have successfully decomposed the covariance matrix into eigenpairs, let's now proceed with the last three steps to transform the *Wine* dataset onto the new principal component axes. The remaining steps we are going to tackle in this section are the following ones:\n",
    "\n",
    "- Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where *k* is the dimensionality of the new feature subspace (*k*≤*d*).\n",
    "\n",
    "    $$k \\le d$$\n",
    "\n",
    "- Construct a projection matrix ***W*** from the \"top\" *k* eigenvectors.\n",
    "- Transform the *d*-dimensional input dataset ***X*** using the projection matrix ***W*** to obtain the new *k*-dimensional feature subspace.\n",
    "\n",
    "Or, in less technical terms, we will sort the eigenpairs by descending order of the eigenvalues, construct a projection matrix from the selected eigenvectors, and use the projection matrix to transform the data onto the lower-dimensional subspace.\n",
    "\n",
    "We start by sorting the eigenpairs by decreasing order of the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
