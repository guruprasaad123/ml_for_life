{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5,name=\"X\")\n",
    "y = tf.Variable(7,name=\"Y\")\n",
    "f=(x*x*y) + y +2\n",
    "f2=(x*x*y)\n",
    "result =None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 0\n",
    "## Using Initializer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =>  Tensor(\"X/read:0\", shape=(), dtype=int32)\n",
      "Y =>  Tensor(\"Y/read:0\", shape=(), dtype=int32)\n",
      "result =>  184\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "\n",
    "print('X => ', x.value())\n",
    "\n",
    "sess.run(y.initializer)\n",
    "\n",
    "print('Y => ',y.value())\n",
    "result = sess.run(f)\n",
    "\n",
    "print('result => ',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "## Intializing the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =>  5\n",
      "y =>  7\n",
      "eval =>  184\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run() #tf.get_default_session.run(x.initializer)\n",
    "    print('x => ',x.eval())\n",
    "    y.initializer.run()\n",
    "    print('y => ',y.eval())\n",
    "    result=f.eval()     #tf.get_default_session.run(f)\n",
    "    print('eval => ',f.eval())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "## Using Global Variables Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result  184\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # handles all the initialisation of x , y\n",
    "    result = f.eval()\n",
    "    print('result ',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3\n",
    "## InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is same graph =  True\n",
      "is same graph =  False\n",
      "is same graph =  True\n",
      "184\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result1,result2 = sess.run([f,f2]) #(x*x*y) will be reused\n",
    "sess.close()\n",
    "\n",
    "\n",
    "x1=  tf.Variable(2)\n",
    "same_graph = x1.graph is tf.get_default_graph()\n",
    "\n",
    "print('is same graph = ',same_graph)\n",
    "\n",
    "x2=None\n",
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "    x2=tf.Variable(2)\n",
    "\n",
    "same_graph  = x2.graph is tf.get_default_graph()\n",
    "\n",
    "print('is same graph = ',same_graph)\n",
    "\n",
    "same_graph  = x2.graph is graph_1\n",
    "\n",
    "print('is same graph = ',same_graph)\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing linear Regression - Normal Equations\n",
    "\n",
    "$$ \\theta = (X^T . X )^{-1} . X^T.Y  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result =>  [[-3.6894890e+01]\n",
      " [ 4.3661433e-01]\n",
      " [ 9.4453208e-03]\n",
      " [-1.0704148e-01]\n",
      " [ 6.4345831e-01]\n",
      " [-3.9632569e-06]\n",
      " [-3.7880042e-03]\n",
      " [-4.2093179e-01]\n",
      " [-4.3400639e-01]]\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "m , n = housing.data.shape\n",
    "\n",
    "data_bias = np.c_[np.ones((m,1)),housing.data]\n",
    "\n",
    "X = tf.constant(data_bias , dtype=tf.float32 , name='X')\n",
    "Y = tf.constant(housing.target.reshape(-1,1) , dtype=tf.float32 , name='Y')\n",
    "\n",
    "X_T = tf.transpose(X)\n",
    "theta = tf.matmul( tf.matmul( tf.matrix_inverse( tf.matmul(X_T ,X) ) , X_T ) , Y )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(theta)\n",
    "    print('result => ',result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/buckaroo/nlp/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:2618: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "MSE = 8.900891304016113\n",
      "MSE = 12.318581581115723\n",
      "MSE = 47.59954833984375\n",
      "MSE = 114.7437973022461\n",
      "MSE = 213.75149536132812\n",
      "MSE = 344.62255859375\n",
      "MSE = 507.35662841796875\n",
      "MSE = 701.953857421875\n",
      "MSE = 928.4133911132812\n",
      "MSE = 1186.7379150390625\n",
      "Reduced MSE = 1476.9266357421875\n",
      "Best m = [[ 19.412827 ]\n",
      " [ 19.086464 ]\n",
      " [  3.5055757]\n",
      " [ -2.7687204]\n",
      " [-10.973658 ]\n",
      " [  8.626957 ]\n",
      " [  6.911361 ]\n",
      " [-13.404756 ]] ,b = [[-0.2848785]]\n"
     ]
    }
   ],
   "source": [
    "housing_data = fetch_california_housing()\n",
    "\n",
    "m_,n=housing_data.data.shape\n",
    "\n",
    "housing_data_plus_bias = np.c_[np.ones((m_,1)),housing_data.data]\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# scalar.fit(housing_data_plus_bias)\n",
    "\n",
    "scaled_housing_data_plus_bias = scalar.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "scaled_housing_data = scalar.fit_transform(housing_data.data)\n",
    "\n",
    "# print(scaled_housing_data_plus_bias)\n",
    "\n",
    "# print(housing_data_plus_bias)\n",
    "\n",
    "# print(housing_data_plus_bias.shape)\n",
    "\n",
    "learning_rate=0.01\n",
    "epochs = 1000\n",
    "\n",
    "x = tf.constant(scaled_housing_data , dtype=tf.float32 , name='x')\n",
    "\n",
    "y = tf.constant(housing_data.target.reshape(-1,1) , dtype=tf.float32 , name='y')\n",
    "\n",
    "x_t = tf.transpose(x)\n",
    "# theta = tf.matmul(tf.matrix_inverse(tf.matmul(x_t,x)),tf.matmul(x_t,y))\n",
    "\n",
    "# theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")\n",
    "\n",
    "m = tf.Variable(tf.random_uniform([n,1],-1.0,1.0),name=\"m\")\n",
    "b = tf.Variable(tf.random_uniform([1,1],-1.0,1.0),name=\"b\")\n",
    "\n",
    "y_pred= tf.matmul(x,m)+b\n",
    "\n",
    "error = tf.square(y_pred - y)\n",
    "mse=tf.reduce_mean(error,name='mse')\n",
    "\n",
    "#y_ = (tf.matmul(x,m)+b)\n",
    "\n",
    "m_gradients = tf.Variable(-2/m_ * tf.matmul(x_t,y-y_pred))\n",
    "\n",
    "b_gradients = tf.Variable((-2/m_) * tf.reduce_mean(y-y_pred))\n",
    "\n",
    "# m_gradients_upd = tf.assign(m_gradients,m_gradients)\n",
    "\n",
    "# b_gradients_upd = tf.assign(b_gradients,b_gradients)\n",
    "\n",
    "\n",
    "m_ops = tf.assign(m, m- (learning_rate * m_gradients))\n",
    "b_ops = tf.assign(b, b- (learning_rate * b_gradients))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 100 == 0 :\n",
    "            print(\"MSE = {}\".format(mse.eval()))\n",
    "        sess.run([m_ops,b_ops])\n",
    "    \n",
    "    print(\"Reduced MSE = {}\".format(mse.eval()))\n",
    "    print(\"Best m = {} ,b = {}\".format(m.eval(),b.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - using tensorflow Autodiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 11.038325309753418\n",
      "MSE = 0.8555397391319275\n",
      "MSE = 0.6099303960800171\n",
      "MSE = 0.5811888575553894\n",
      "MSE = 0.5657209753990173\n",
      "MSE = 0.554740309715271\n",
      "MSE = 0.5467424392700195\n",
      "MSE = 0.5408945679664612\n",
      "MSE = 0.5366097092628479\n",
      "MSE = 0.5334633588790894\n",
      "Reduced MSE = 0.531147301197052\n",
      "Best m = [[ 0.8344455 ]\n",
      " [ 0.1477353 ]\n",
      " [-0.22212528]\n",
      " [ 0.24670917]\n",
      " [ 0.00597718]\n",
      " [-0.0418807 ]\n",
      " [-0.68405956]\n",
      " [-0.6527358 ]] ,b = [[2.0685523]]\n"
     ]
    }
   ],
   "source": [
    "m_,n=housing_data.data.shape\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data = scalar.fit_transform(housing_data.data)\n",
    "\n",
    "learning_rate=0.01\n",
    "epochs = 1000\n",
    "\n",
    "x = tf.constant(scaled_housing_data ,dtype=tf.float32,name='x')\n",
    "y = tf.constant(housing_data.target.reshape(-1,1) ,dtype=tf.float32,name='y')\n",
    "x_t = tf.transpose(x)\n",
    "\n",
    "m = tf.Variable(tf.random_uniform([n,1],-1.0,1.0),name=\"m\")\n",
    "b = tf.Variable(tf.random_uniform([1,1],-1.0,1.0),name=\"b\")\n",
    "\n",
    "y_pred= tf.matmul(x,m)+b\n",
    "\n",
    "error = tf.square(y_pred - y)\n",
    "mse=tf.reduce_mean(error,name='mse')\n",
    "\n",
    "m_gradients = tf.gradients(mse , [m])[0]\n",
    "\n",
    "b_gradients = tf.gradients(mse , [b])[0]\n",
    "\n",
    "m_ops = tf.assign(m, m- (learning_rate * m_gradients))\n",
    "b_ops = tf.assign(b, b- (learning_rate * b_gradients))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 100 == 0 :\n",
    "            print(\"MSE = {}\".format(mse.eval()))\n",
    "        sess.run([m_ops,b_ops])\n",
    "    \n",
    "    print(\"Reduced MSE = {}\".format(mse.eval()))\n",
    "    print(\"Best m = {} ,b = {}\".format(m.eval(),b.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Using Optimizers\n",
    "\n",
    "    - GradientDescent Optimizer\n",
    "    - Momentem Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 6.132932662963867\n",
      "MSE = 0.7304072380065918\n",
      "MSE = 0.6216009855270386\n",
      "MSE = 0.6010381579399109\n",
      "MSE = 0.5862542390823364\n",
      "MSE = 0.5745306611061096\n",
      "MSE = 0.5651532411575317\n",
      "MSE = 0.557619571685791\n",
      "MSE = 0.5515443086624146\n",
      "MSE = 0.5466277003288269\n",
      "Reduced MSE = 0.5426360368728638\n",
      "Best m = [[ 0.95084774]\n",
      " [ 0.15765905]\n",
      " [-0.46430957]\n",
      " [ 0.45676467]\n",
      " [ 0.00807702]\n",
      " [-0.04520228]\n",
      " [-0.5024195 ]\n",
      " [-0.48572063]] ,b = [[2.0685523]]\n"
     ]
    }
   ],
   "source": [
    "m_,n=housing_data.data.shape\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data = scalar.fit_transform(housing_data.data)\n",
    "\n",
    "learning_rate=0.01\n",
    "epochs = 1000\n",
    "\n",
    "x = tf.constant(scaled_housing_data ,dtype=tf.float32,name='x')\n",
    "y = tf.constant(housing_data.target.reshape(-1,1) ,dtype=tf.float32,name='y')\n",
    "x_t = tf.transpose(x)\n",
    "\n",
    "m = tf.Variable(tf.random_uniform([n,1],-1.0,1.0),name=\"m\")\n",
    "b = tf.Variable(tf.random_uniform([1,1],-1.0,1.0),name=\"b\")\n",
    "\n",
    "y_pred= tf.matmul(x,m)+b\n",
    "\n",
    "error = tf.square(y_pred - y)\n",
    "\n",
    "# Optimiser \n",
    "\n",
    "# learning_rate = 0.01;\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "mse = tf.reduce_mean(error,name='mse')\n",
    "\n",
    "'''\n",
    "m_gradients = tf.gradients(mse , [m])[0]\n",
    "\n",
    "b_gradients = tf.gradients(mse , [b])[0]\n",
    "\n",
    "m_ops = tf.assign(m, m- (learning_rate * m_gradients))\n",
    "\n",
    "b_ops = tf.assign(b, b- (learning_rate * b_gradients))\n",
    "\n",
    "'''\n",
    "\n",
    "m_ops = optimizer.minimize(mse,var_list=[m]) \n",
    "\n",
    "b_ops = optimizer.minimize(mse,var_list=[b]) \n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 100 == 0 :\n",
    "            print(\"MSE = {}\".format(mse.eval()))\n",
    "        sess.run([m_ops,b_ops])\n",
    "    \n",
    "    print(\"Reduced MSE = {}\".format(mse.eval()))\n",
    "    print(\"Best m = {} ,b = {}\".format(m.eval(),b.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 14.642826080322266\n",
      "MSE = 0.5302744507789612\n",
      "MSE = 0.5244543552398682\n",
      "MSE = 0.5243276357650757\n",
      "MSE = 0.5243217349052429\n",
      "MSE = 0.5243210196495056\n",
      "MSE = 0.5243209600448608\n",
      "MSE = 0.5243210196495056\n",
      "MSE = 0.5243210196495056\n",
      "MSE = 0.5243210196495056\n",
      "Reduced MSE = 0.5243210196495056\n",
      "Best m = [[ 0.82962173]\n",
      " [ 0.11875207]\n",
      " [-0.26553148]\n",
      " [ 0.3057    ]\n",
      " [-0.00450285]\n",
      " [-0.03932637]\n",
      " [-0.89988035]\n",
      " [-0.870536  ]] ,b = [[2.068558]]\n"
     ]
    }
   ],
   "source": [
    "m_,n=housing_data.data.shape\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data = scalar.fit_transform(housing_data.data)\n",
    "\n",
    "learning_rate=0.01\n",
    "epochs = 1000\n",
    "\n",
    "x = tf.constant(scaled_housing_data ,dtype=tf.float32,name='x')\n",
    "y = tf.constant(housing_data.target.reshape(-1,1) ,dtype=tf.float32,name='y')\n",
    "x_t = tf.transpose(x)\n",
    "\n",
    "m = tf.Variable(tf.random_uniform([n,1],-1.0,1.0),name=\"m\")\n",
    "b = tf.Variable(tf.random_uniform([1,1],-1.0,1.0),name=\"b\")\n",
    "\n",
    "y_pred= tf.matmul(x,m)+b\n",
    "\n",
    "error = tf.square(y_pred - y)\n",
    "\n",
    "# Optimiser \n",
    "\n",
    "# learning_rate = 0.01;\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=0.9)\n",
    "\n",
    "mse = tf.reduce_mean(error,name='mse')\n",
    "\n",
    "'''\n",
    "m_gradients = tf.gradients(mse , [m])[0]\n",
    "\n",
    "b_gradients = tf.gradients(mse , [b])[0]\n",
    "\n",
    "m_ops = tf.assign(m, m- (learning_rate * m_gradients))\n",
    "\n",
    "b_ops = tf.assign(b, b- (learning_rate * b_gradients))\n",
    "\n",
    "'''\n",
    "\n",
    "m_ops = optimizer.minimize(mse,var_list=[m]) \n",
    "\n",
    "b_ops = optimizer.minimize(mse,var_list=[b]) \n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 100 == 0 :\n",
    "            print(\"MSE = {}\".format(mse.eval()))\n",
    "        sess.run([m_ops,b_ops])\n",
    "    \n",
    "    print(\"Reduced MSE = {}\".format(mse.eval()))\n",
    "    print(\"Best m = {} ,b = {}\".format(m.eval(),b.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best m = [[6.1880487e+18]\n",
      " [6.1270176e+18]\n",
      " [2.5369268e+18]\n",
      " [4.2534222e+18]\n",
      " [9.9428221e+18]\n",
      " [4.9338939e+20]\n",
      " [1.3064870e+19]\n",
      " [5.5675333e+18]] ,b = [[7.3539197e+18]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "m_,n=housing_data.data.shape\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data = scalar.fit_transform(housing_data.data)\n",
    "\n",
    "learning_rate=0.01\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int( np.ceil( m_ / batch_size) ) \n",
    "\n",
    "x = tf.placeholder(tf.float32 , shape=(None,n) , name='x' )\n",
    "y = tf.placeholder(tf.float32 , shape=(None,1) , name='y' )\n",
    "\n",
    "x_t = tf.transpose(x)\n",
    "\n",
    "m = tf.Variable(tf.random_uniform([n,1],-1.0,1.0),name=\"m\")\n",
    "b = tf.Variable(tf.random_uniform([1,1],-1.0,1.0),name=\"b\")\n",
    "\n",
    "y_pred= tf.matmul(x,m)+b\n",
    "\n",
    "error = tf.square(y_pred - y)\n",
    "mse=tf.reduce_mean(error,name='mse')\n",
    "\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=0.9)\n",
    "\n",
    "mse = tf.reduce_mean(error,name='mse')\n",
    "\n",
    "'''\n",
    "m_gradients = tf.gradients(mse , [m])[0]\n",
    "\n",
    "b_gradients = tf.gradients(mse , [b])[0]\n",
    "\n",
    "m_ops = tf.assign(m, m- (learning_rate * m_gradients))\n",
    "\n",
    "b_ops = tf.assign(b, b- (learning_rate * b_gradients))\n",
    "\n",
    "'''\n",
    "\n",
    "m_ops = optimizer.minimize(mse,var_list=[m]) \n",
    "\n",
    "b_ops = optimizer.minimize(mse,var_list=[b]) \n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m_, size=batch_size) # not shown\n",
    "    X_batch = scaled_housing_data[indices]           # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_val = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            # print( \"x = {} , y = {}\".format(X_batch.shape , y_batch.shape) )\n",
    "            \n",
    "            try:\n",
    "                mse_val , m_val , b_val = sess.run([mse,m_ops,b_ops], feed_dict={x: X_batch, y: y_batch})\n",
    "                \n",
    "                \n",
    "            except e :\n",
    "                print('error ',e)\n",
    "                print(X_batch,y_batch)\n",
    "\n",
    "    print(\"Best m = {} ,b = {}\".format(m.eval(),b.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n batches = 207\n",
      "best_theta  [[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "m_,n=housing_data.data.shape\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m_, 1)), scaled_housing_data]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m_ / batch_size))\n",
    "\n",
    "print('n batches = {}'.format(n_batches))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m_, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    print( 'best_theta ',best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model & Restore\n",
    "- Save\n",
    "- Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 6.450078010559082\n",
      "MSE = 0.5273270010948181\n",
      "MSE = 0.5244448184967041\n",
      "MSE = 0.5243327617645264\n",
      "MSE = 0.5243224501609802\n",
      "MSE = 0.5243211388587952\n",
      "MSE = 0.5243210196495056\n",
      "MSE = 0.5243210196495056\n",
      "MSE = 0.5243209600448608\n",
      "MSE = 0.5243210196495056\n",
      "Reduced MSE = 0.5243210196495056\n",
      "Best m = [[ 0.8296228 ]\n",
      " [ 0.11875226]\n",
      " [-0.26553345]\n",
      " [ 0.3057016 ]\n",
      " [-0.00450283]\n",
      " [-0.0393264 ]\n",
      " [-0.899878  ]\n",
      " [-0.8705338 ]] ,b = [[2.068558]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "m_,n=housing_data.data.shape\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data = scalar.fit_transform(housing_data.data)\n",
    "\n",
    "learning_rate=0.01\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "x = tf.constant(scaled_housing_data ,dtype=tf.float32,name='x')\n",
    "y = tf.constant(housing_data.target.reshape(-1,1) ,dtype=tf.float32,name='y')\n",
    "x_t = tf.transpose(x)\n",
    "\n",
    "m = tf.Variable(tf.random_uniform([n,1],-1.0,1.0),name=\"m\")\n",
    "b = tf.Variable(tf.random_uniform([1,1],-1.0,1.0),name=\"b\")\n",
    "\n",
    "y_pred= tf.matmul(x,m)+b\n",
    "\n",
    "error = tf.square(y_pred - y)\n",
    "\n",
    "# Optimiser \n",
    "\n",
    "# learning_rate = 0.01;\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=0.9)\n",
    "\n",
    "mse = tf.reduce_mean(error,name='mse')\n",
    "\n",
    "'''\n",
    "m_gradients = tf.gradients(mse , [m])[0]\n",
    "\n",
    "b_gradients = tf.gradients(mse , [b])[0]\n",
    "\n",
    "m_ops = tf.assign(m, m- (learning_rate * m_gradients))\n",
    "\n",
    "b_ops = tf.assign(b, b- (learning_rate * b_gradients))\n",
    "\n",
    "'''\n",
    "\n",
    "m_ops = optimizer.minimize(mse,var_list=[m]) \n",
    "\n",
    "b_ops = optimizer.minimize(mse,var_list=[b]) \n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver to save the session\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 100 == 0 :\n",
    "            print(\"MSE = {}\".format(mse.eval()))\n",
    "        sess.run([m_ops,b_ops])\n",
    "        sess_path = saver.save(sess,'temp/my_model.ckpt')\n",
    "    \n",
    "    print(\"Reduced MSE = {}\".format(mse.eval()))\n",
    "\n",
    "    m_val = m.eval()\n",
    "    b_val = b.eval()\n",
    "    print(\"Best m = {} ,b = {}\".format(m_val,b_val))\n",
    "    \n",
    "    sess_path = saver.save(sess,'temp/my_model_final.ckpt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/buckaroo/nlp/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from temp/my_model_final.ckpt\n",
      " m is same = True , b is same = True \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"temp/my_model_final.ckpt\")\n",
    "    # sess.run(init)\n",
    "    m_restored = m.eval() # not shown in the book\n",
    "    b_restored = b.eval() \n",
    "    \n",
    "\n",
    "# to check whether the restored value is same as the saved one\n",
    "print(\" m is same = {} , b is same = {} \".format( np.allclose(m_val,m_restored) , np.allclose(b_val,b_restored) )  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using Logistic Regression with Moons Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2) (200, 2) (800, 1) (200, 1)\n",
      "(800, 2000) (200, 2000) (800, 2) (200, 1)\n",
      "(1000, 2000) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons[:-test_size]\n",
    "X_test = X_moons[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]\n",
    "\n",
    "# Creating the One Hot Encoder \n",
    "oneHot = OneHotEncoder() \n",
    "  \n",
    "# Encoding \n",
    "\n",
    "oneHot.fit(X_moons)\n",
    "X_moons_hot = oneHot.transform(X_moons).toarray()\n",
    "\n",
    "oneHot.fit(y_moons_column_vector)\n",
    "y_moons_hot = oneHot.transform(y_moons_column_vector).toarray()\n",
    "\n",
    "X_train_hot = X_moons_hot[:-test_size]\n",
    "X_test_hot = X_moons_hot[-test_size:]\n",
    "y_train_hot = y_moons_hot[:-test_size]\n",
    "y_test_hot = y_moons_hot[-test_size:]\n",
    "\n",
    "print( X_train.shape , X_test.shape , y_train.shape , y_test.shape )\n",
    "print( X_train_hot.shape , X_test_hot.shape , y_train_hot.shape , y_test.shape)\n",
    "print( X_moons_hot.shape , y_moons_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get Random N rows X , Y batches\n",
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 2000\n"
     ]
    }
   ],
   "source": [
    "m , n = X_train_hot.shape\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "# There are n columns in the feature matrix \n",
    "# after One Hot Encoding. \n",
    "X = tf.placeholder(tf.float32, [None, n]) \n",
    "  \n",
    "# Since this is a binary classification problem, \n",
    "# Y can take only 2 values. \n",
    "Y = tf.placeholder(tf.float32, [None, 2]) \n",
    "  \n",
    "# Trainable Variable Weights \n",
    "W = tf.Variable(tf.zeros([n, 2])) \n",
    "  \n",
    "# Trainable Variable Bias \n",
    "b = tf.Variable(tf.zeros([2])) \n",
    "\n",
    "# Hypothesis \n",
    "Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(X, W), b)) \n",
    "  \n",
    "# Sigmoid Cross Entropy Cost Function \n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_hat, labels = Y) \n",
    "  \n",
    "# Gradient Descent Optimizer \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost) \n",
    "  \n",
    "# Global Variables Initializer \n",
    "init = tf.global_variables_initializer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the Tensorflow Session \n",
    "with tf.Session() as sess: \n",
    "      \n",
    "    # Initializing the Variables \n",
    "    sess.run(init) \n",
    "      \n",
    "    # Lists for storing the changing Cost and Accuracy in every Epoch \n",
    "    cost_history, accuracy_history = [], [] \n",
    "      \n",
    "    # Iterating through all the epochs \n",
    "    for epoch in range(epochs): \n",
    "        cost_per_epoch = 0\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_hot, y_train_hot, batch_size)\n",
    "          \n",
    "            # Running the Optimizer \n",
    "            sess.run(optimizer, feed_dict = { X : X_batch , Y : y_batch }) \n",
    "          \n",
    "            # Calculating cost on current Epoch \n",
    "            c = sess.run(cost, feed_dict = {  X : X_batch , Y : y_batch }) \n",
    "            \n",
    "            \n",
    "          \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: [[0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.93553376 0.4941156 ]\n",
      " [0.49819964 0.9418913 ]\n",
      " [0.49819964 0.9418913 ]]\n",
      "Epoch: 100 \tLoss: [[0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]\n",
      " [0.76307446 0.62465423]\n",
      " [0.6277917  0.76667863]\n",
      " [0.6277917  0.76667863]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 \tLoss: [[0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.8141376  0.56703347]\n",
      " [0.5852282  0.8374924 ]\n",
      " [0.5852282  0.8374924 ]]\n",
      "Epoch: 300 \tLoss: [[0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]\n",
      " [0.85903776 0.5472911 ]\n",
      " [0.5509005  0.86397076]\n",
      " [0.5509005  0.86397076]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400 \tLoss: [[0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.8822634  0.53551555]\n",
      " [0.5341713  0.8803626 ]\n",
      " [0.5341713  0.8803626 ]]\n",
      "Epoch: 500 \tLoss: [[0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.88696694 0.52215326]\n",
      " [0.5308637  0.8995362 ]\n",
      " [0.5308637  0.8995362 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600 \tLoss: [[0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]\n",
      " [0.8919817  0.5151031 ]\n",
      " [0.5273663  0.90990865]\n",
      " [0.5273663  0.90990865]]\n",
      "Epoch: 700 \tLoss: [[0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]\n",
      " [0.9034729  0.51593935]\n",
      " [0.51946306 0.90866876]\n",
      " [0.51946306 0.90866876]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 800 \tLoss: [[0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]\n",
      " [0.9117892  0.5165504 ]\n",
      " [0.51383793 0.90776455]\n",
      " [0.51383793 0.90776455]]\n",
      "Epoch: 900 \tLoss: [[0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.9113021  0.51260096]\n",
      " [0.5141653  0.9136336 ]\n",
      " [0.5141653  0.9136336 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_hot , y_train_hot, batch_size)\n",
    "            sess.run(optimizer, feed_dict={X: X_batch, Y: y_batch})\n",
    "        loss_val = cost.eval({X: X_test_hot, Y: y_test_hot})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = Y_hat.eval(feed_dict={X: X_test_hot, Y: y_test_hot})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
