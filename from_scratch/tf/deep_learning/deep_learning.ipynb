{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.fully_connected()` rather than `tf.layers.dense()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dense()`, because anything in the contrib module may change or be deleted without notice. The `dense()` function is almost identical to the `fully_connected()` function. The main differences relevant to this chapter are:\n",
    "* several parameters are renamed: `scope` becomes `name`, `activation_fn` becomes `activation` (and similarly the `_fn` suffix is removed from other parameters such as `normalizer_fn`), `weights_initializer` becomes `kernel_initializer`, etc.\n",
    "* the default `activation` is now `None` rather than `tf.nn.relu`.\n",
    "* it does not support `tensorflow.contrib.framework.arg_scope()` (introduced later in chapter 11).\n",
    "* it does not support regularizer params (introduced later in chapter 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-50dd995eb8d1>:12: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/buckaroo/miniconda3/envs/tensorflow-legacy/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return tf.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/buckaroo/miniconda3/envs/tensorflow-legacy/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data: <br>\n",
    "**Warning**: `tf.examples.tutorials.mnist` is deprecated. We will use `tf.keras.datasets.mnist` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9046\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9492\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9654\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.9712\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9766\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9776\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing `ELU` in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.9036\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9338\n",
      "10 Batch accuracy: 0.9 Validation accuracy: 0.9532\n",
      "15 Batch accuracy: 0.9 Validation accuracy: 0.9622\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9656\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9698\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9698\n",
      "35 Batch accuracy: 0.98 Validation accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.         -4.94974874 -4.89949749 -4.84924623 -4.79899497 -4.74874372\n",
      " -4.69849246 -4.64824121 -4.59798995 -4.54773869 -4.49748744 -4.44723618\n",
      " -4.39698492 -4.34673367 -4.29648241 -4.24623116 -4.1959799  -4.14572864\n",
      " -4.09547739 -4.04522613 -3.99497487 -3.94472362 -3.89447236 -3.84422111\n",
      " -3.79396985 -3.74371859 -3.69346734 -3.64321608 -3.59296482 -3.54271357\n",
      " -3.49246231 -3.44221106 -3.3919598  -3.34170854 -3.29145729 -3.24120603\n",
      " -3.19095477 -3.14070352 -3.09045226 -3.04020101 -2.98994975 -2.93969849\n",
      " -2.88944724 -2.83919598 -2.78894472 -2.73869347 -2.68844221 -2.63819095\n",
      " -2.5879397  -2.53768844 -2.48743719 -2.43718593 -2.38693467 -2.33668342\n",
      " -2.28643216 -2.2361809  -2.18592965 -2.13567839 -2.08542714 -2.03517588\n",
      " -1.98492462 -1.93467337 -1.88442211 -1.83417085 -1.7839196  -1.73366834\n",
      " -1.68341709 -1.63316583 -1.58291457 -1.53266332 -1.48241206 -1.4321608\n",
      " -1.38190955 -1.33165829 -1.28140704 -1.23115578 -1.18090452 -1.13065327\n",
      " -1.08040201 -1.03015075 -0.9798995  -0.92964824 -0.87939698 -0.82914573\n",
      " -0.77889447 -0.72864322 -0.67839196 -0.6281407  -0.57788945 -0.52763819\n",
      " -0.47738693 -0.42713568 -0.37688442 -0.32663317 -0.27638191 -0.22613065\n",
      " -0.1758794  -0.12562814 -0.07537688 -0.02512563  0.02512563  0.07537688\n",
      "  0.12562814  0.1758794   0.22613065  0.27638191  0.32663317  0.37688442\n",
      "  0.42713568  0.47738693  0.52763819  0.57788945  0.6281407   0.67839196\n",
      "  0.72864322  0.77889447  0.82914573  0.87939698  0.92964824  0.9798995\n",
      "  1.03015075  1.08040201  1.13065327  1.18090452  1.23115578  1.28140704\n",
      "  1.33165829  1.38190955  1.4321608   1.48241206  1.53266332  1.58291457\n",
      "  1.63316583  1.68341709  1.73366834  1.7839196   1.83417085  1.88442211\n",
      "  1.93467337  1.98492462  2.03517588  2.08542714  2.13567839  2.18592965\n",
      "  2.2361809   2.28643216  2.33668342  2.38693467  2.43718593  2.48743719\n",
      "  2.53768844  2.5879397   2.63819095  2.68844221  2.73869347  2.78894472\n",
      "  2.83919598  2.88944724  2.93969849  2.98994975  3.04020101  3.09045226\n",
      "  3.14070352  3.19095477  3.24120603  3.29145729  3.34170854  3.3919598\n",
      "  3.44221106  3.49246231  3.54271357  3.59296482  3.64321608  3.69346734\n",
      "  3.74371859  3.79396985  3.84422111  3.89447236  3.94472362  3.99497487\n",
      "  4.04522613  4.09547739  4.14572864  4.1959799   4.24623116  4.29648241\n",
      "  4.34673367  4.39698492  4.44723618  4.49748744  4.54773869  4.59798995\n",
      "  4.64824121  4.69849246  4.74874372  4.79899497  4.84924623  4.89949749\n",
      "  4.94974874  5.        ]\n",
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "print(z)\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.selu()` function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=alpha_0_1, alpha=scale_0_1):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the SELU activation function cannot be used along with regular Dropout (this would cancel the SELU activation function's self-normalizing property). Fortunately, there is a Dropout variant called Alpha Dropout proposed in the same paper. It is available in `tf.contrib.nn.alpha_dropout()` since TF 1.4 (or check out [this implementation](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) by the Institute of Bioinformatics, Johannes Kepler University Linz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.94 Validation accuracy: 0.9388\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9634\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9706\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9692\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9692\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.971\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.batch_norm()` rather than `tf.layers.batch_normalization()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.batch_normalization()`, because anything in the contrib module may change or be deleted without notice. Instead of using the `batch_norm()` function as a regularizer parameter to the `fully_connected()` function, we now use `batch_normalization()` and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n",
    "* `decay` is renamed to `momentum`,\n",
    "* `is_training` is renamed to `training`,\n",
    "* `updates_collections` is removed: the update operations needed by batch normalization are added to the `UPDATE_OPS` collection and you need to explicity run these operations during training (see the execution phase below),\n",
    "* we don't need to specify `scale=True`, as that is the default.\n",
    "\n",
    "Also note that in order to run batch norm just _before_ each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "Note: since the `tf.layers.dense()` function is incompatible with `tf.contrib.layers.arg_scope()` (which is used in the book), we now use python's `functools.partial()` function instead. It makes it easy to create a `my_dense_layer()` function that just calls `tf.layers.dense()` with the desired parameters automatically set (unless they are overridden when calling `my_dense_layer()`). As you can see, the code remains very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over again, we can use Python's `partial()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are using `tf.layers.batch_normalization()` rather than `tf.contrib.layers.batch_norm()` (as in the book), we need to explicitly run the extra update operations needed by batch normalization (`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8998\n",
      "1 Validation accuracy: 0.9188\n",
      "2 Validation accuracy: 0.9316\n",
      "3 Validation accuracy: 0.943\n",
      "4 Validation accuracy: 0.9462\n",
      "5 Validation accuracy: 0.9502\n",
      "6 Validation accuracy: 0.956\n",
      "7 Validation accuracy: 0.9574\n",
      "8 Validation accuracy: 0.9618\n",
      "9 Validation accuracy: 0.965\n",
      "10 Validation accuracy: 0.966\n",
      "11 Validation accuracy: 0.9672\n",
      "12 Validation accuracy: 0.9682\n",
      "13 Validation accuracy: 0.9704\n",
      "14 Validation accuracy: 0.9712\n",
      "15 Validation accuracy: 0.9704\n",
      "16 Validation accuracy: 0.9726\n",
      "17 Validation accuracy: 0.9722\n",
      "18 Validation accuracy: 0.9748\n",
      "19 Validation accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.\n",
    "\n",
    "Note that you could also make the training operation depend on the update operations:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "This way, you would just have to evaluate the `training_op` during training, TensorFlow would automatically run the update operations as well:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```\n",
    "\n",
    "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.288\n",
      "1 Validation accuracy: 0.7944\n",
      "2 Validation accuracy: 0.8798\n",
      "3 Validation accuracy: 0.906\n",
      "4 Validation accuracy: 0.9164\n",
      "5 Validation accuracy: 0.922\n",
      "6 Validation accuracy: 0.9292\n",
      "7 Validation accuracy: 0.9358\n",
      "8 Validation accuracy: 0.9382\n",
      "9 Validation accuracy: 0.9414\n",
      "10 Validation accuracy: 0.9456\n",
      "11 Validation accuracy: 0.947\n",
      "12 Validation accuracy: 0.9476\n",
      "13 Validation accuracy: 0.9534\n",
      "14 Validation accuracy: 0.9566\n",
      "15 Validation accuracy: 0.9566\n",
      "16 Validation accuracy: 0.9578\n",
      "17 Validation accuracy: 0.9588\n",
      "18 Validation accuracy: 0.9622\n",
      "19 Validation accuracy: 0.9616\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that you can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a `FileWriter` to save the graph and then visualize it in TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_graph_in_jupyter import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/e/workspace/@Machine_learning/ml_for_life/from_scratch/tf/deep_learning/tensorflow_graph_in_jupyter.py:18: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/filename/input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/filename&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;save/filename/input&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;save/filename&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a session, restore the model's state and continue training on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let's test this for real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9634\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9654\n",
      "3 Validation accuracy: 0.965\n",
      "4 Validation accuracy: 0.964\n",
      "5 Validation accuracy: 0.9648\n",
      "6 Validation accuracy: 0.9688\n",
      "7 Validation accuracy: 0.9684\n",
      "8 Validation accuracy: 0.9684\n",
      "9 Validation accuracy: 0.9682\n",
      "10 Validation accuracy: 0.9702\n",
      "11 Validation accuracy: 0.9712\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9708\n",
      "15 Validation accuracy: 0.9722\n",
      "16 Validation accuracy: 0.972\n",
      "17 Validation accuracy: 0.9712\n",
      "18 Validation accuracy: 0.9714\n",
      "19 Validation accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9644\n",
      "1 Validation accuracy: 0.9628\n",
      "2 Validation accuracy: 0.9654\n",
      "3 Validation accuracy: 0.965\n",
      "4 Validation accuracy: 0.9642\n",
      "5 Validation accuracy: 0.9648\n",
      "6 Validation accuracy: 0.9686\n",
      "7 Validation accuracy: 0.9682\n",
      "8 Validation accuracy: 0.9682\n",
      "9 Validation accuracy: 0.9686\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.9716\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9726\n",
      "16 Validation accuracy: 0.972\n",
      "17 Validation accuracy: 0.9712\n",
      "18 Validation accuracy: 0.9714\n",
      "19 Validation accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you will want to reuse only the lower layers. If you are using `import_meta_graph()` it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9188\n",
      "1 Validation accuracy: 0.9392\n",
      "2 Validation accuracy: 0.9492\n",
      "3 Validation accuracy: 0.9524\n",
      "4 Validation accuracy: 0.9556\n",
      "5 Validation accuracy: 0.9562\n",
      "6 Validation accuracy: 0.9574\n",
      "7 Validation accuracy: 0.961\n",
      "8 Validation accuracy: 0.9612\n",
      "9 Validation accuracy: 0.964\n",
      "10 Validation accuracy: 0.9652\n",
      "11 Validation accuracy: 0.966\n",
      "12 Validation accuracy: 0.9634\n",
      "13 Validation accuracy: 0.9672\n",
      "14 Validation accuracy: 0.968\n",
      "15 Validation accuracy: 0.9682\n",
      "16 Validation accuracy: 0.9694\n",
      "17 Validation accuracy: 0.9674\n",
      "18 Validation accuracy: 0.9696\n",
      "19 Validation accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you must create one `Saver` to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another `Saver` to save the new model, once it is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9022\n",
      "1 Validation accuracy: 0.9338\n",
      "2 Validation accuracy: 0.9432\n",
      "3 Validation accuracy: 0.9472\n",
      "4 Validation accuracy: 0.9524\n",
      "5 Validation accuracy: 0.9526\n",
      "6 Validation accuracy: 0.9556\n",
      "7 Validation accuracy: 0.959\n",
      "8 Validation accuracy: 0.9588\n",
      "9 Validation accuracy: 0.9604\n",
      "10 Validation accuracy: 0.9622\n",
      "11 Validation accuracy: 0.962\n",
      "12 Validation accuracy: 0.9638\n",
      "13 Validation accuracy: 0.9662\n",
      "14 Validation accuracy: 0.9664\n",
      "15 Validation accuracy: 0.966\n",
      "16 Validation accuracy: 0.9672\n",
      "17 Validation accuracy: 0.9674\n",
      "18 Validation accuracy: 0.9686\n",
      "19 Validation accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                            # not shown in the book\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Models from Other Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the weights variable created by the `tf.layers.dense()` function is called `\"kernel\"` (instead of `\"weights\"` when using the `tf.contrib.layers.fully_connected()`, as in the book), and the biases variable is called `bias` instead of `biases`.\n",
    "\n",
    "Another approach (initially used in the book) would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.896\n",
      "1 Validation accuracy: 0.9294\n",
      "2 Validation accuracy: 0.94\n",
      "3 Validation accuracy: 0.944\n",
      "4 Validation accuracy: 0.9482\n",
      "5 Validation accuracy: 0.951\n",
      "6 Validation accuracy: 0.9506\n",
      "7 Validation accuracy: 0.9536\n",
      "8 Validation accuracy: 0.9554\n",
      "9 Validation accuracy: 0.9568\n",
      "10 Validation accuracy: 0.956\n",
      "11 Validation accuracy: 0.957\n",
      "12 Validation accuracy: 0.9576\n",
      "13 Validation accuracy: 0.9576\n",
      "14 Validation accuracy: 0.9592\n",
      "15 Validation accuracy: 0.9578\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.9602\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is exactly the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9022\n",
      "1 Validation accuracy: 0.9304\n",
      "2 Validation accuracy: 0.9432\n",
      "3 Validation accuracy: 0.9478\n",
      "4 Validation accuracy: 0.9516\n",
      "5 Validation accuracy: 0.9524\n",
      "6 Validation accuracy: 0.9522\n",
      "7 Validation accuracy: 0.9558\n",
      "8 Validation accuracy: 0.9556\n",
      "9 Validation accuracy: 0.9562\n",
      "10 Validation accuracy: 0.9568\n",
      "11 Validation accuracy: 0.955\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.958\n",
      "14 Validation accuracy: 0.958\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9578\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9022\n",
      "1 Validation accuracy: 0.9304\n",
      "2 Validation accuracy: 0.9432\n",
      "3 Validation accuracy: 0.9478\n",
      "4 Validation accuracy: 0.9516\n",
      "5 Validation accuracy: 0.9524\n",
      "6 Validation accuracy: 0.9522\n",
      "7 Validation accuracy: 0.9558\n",
      "8 Validation accuracy: 0.9556\n",
      "9 Validation accuracy: 0.9562\n",
      "10 Validation accuracy: 0.9568\n",
      "11 Validation accuracy: 0.955\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.958\n",
      "14 Validation accuracy: 0.958\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9578\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # not shown\n",
    "                                                y: y_valid})             # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.949\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9698\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9724\n",
      "15 Batch accuracy: 0.98 Validation accuracy: 0.9658\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9658\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9538\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9598\n",
      "35 Batch accuracy: 0.98 Validation accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_momentum.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.98 Validation accuracy: 0.9526\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.9686\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9718\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9718\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9726\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9726\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9738\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_adagrad.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9, use_nesterov=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9, use_nesterov=True)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.9508\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.963\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.971\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9706\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9608\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.965\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9642\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_nag.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.98 Validation accuracy: 0.9526\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.9686\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9718\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9718\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9726\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9726\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9738\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_RMSProp.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9, use_nesterov=True)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.9508\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9632\n",
      "10 Batch accuracy: 0.98 Validation accuracy: 0.9646\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.967\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9664\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9566\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9582\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9546\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_adam.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9022\n",
      "1 Validation accuracy: 0.9236\n",
      "2 Validation accuracy: 0.935\n",
      "3 Validation accuracy: 0.9392\n",
      "4 Validation accuracy: 0.9444\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is just as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n",
      "10 Validation accuracy: 0.9068\n",
      "11 Validation accuracy: 0.9054\n",
      "12 Validation accuracy: 0.907\n",
      "13 Validation accuracy: 0.9084\n",
      "14 Validation accuracy: 0.9088\n",
      "15 Validation accuracy: 0.9064\n",
      "16 Validation accuracy: 0.9066\n",
      "17 Validation accuracy: 0.9066\n",
      "18 Validation accuracy: 0.9066\n",
      "19 Validation accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a regularization function to the `tf.layers.dense()` function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Python's `partial()` function to avoid repeating the same arguments over and over again. Note that we set the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8274\n",
      "1 Validation accuracy: 0.8766\n",
      "2 Validation accuracy: 0.8952\n",
      "3 Validation accuracy: 0.9016\n",
      "4 Validation accuracy: 0.908\n",
      "5 Validation accuracy: 0.9096\n",
      "6 Validation accuracy: 0.9124\n",
      "7 Validation accuracy: 0.9154\n",
      "8 Validation accuracy: 0.9178\n",
      "9 Validation accuracy: 0.919\n",
      "10 Validation accuracy: 0.92\n",
      "11 Validation accuracy: 0.9224\n",
      "12 Validation accuracy: 0.9212\n",
      "13 Validation accuracy: 0.9228\n",
      "14 Validation accuracy: 0.9224\n",
      "15 Validation accuracy: 0.9216\n",
      "16 Validation accuracy: 0.9218\n",
      "17 Validation accuracy: 0.9228\n",
      "18 Validation accuracy: 0.9216\n",
      "19 Validation accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n",
    "* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n",
    "* the `is_training` parameter is renamed to `training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-7adcf6dce8db>:4: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9246\n",
      "1 Validation accuracy: 0.9452\n",
      "2 Validation accuracy: 0.9484\n",
      "3 Validation accuracy: 0.951\n",
      "4 Validation accuracy: 0.9624\n",
      "5 Validation accuracy: 0.9636\n",
      "6 Validation accuracy: 0.9604\n",
      "7 Validation accuracy: 0.9672\n",
      "8 Validation accuracy: 0.967\n",
      "9 Validation accuracy: 0.9668\n",
      "10 Validation accuracy: 0.9698\n",
      "11 Validation accuracy: 0.9688\n",
      "12 Validation accuracy: 0.969\n",
      "13 Validation accuracy: 0.971\n",
      "14 Validation accuracy: 0.9736\n",
      "15 Validation accuracy: 0.9688\n",
      "16 Validation accuracy: 0.974\n",
      "17 Validation accuracy: 0.9734\n",
      "18 Validation accuracy: 0.9724\n",
      "19 Validation accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the `clip_by_norm()` function. Then we create an assignment operation to assign the clipped weights to the weights variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this as well for the second hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an initializer and a saver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the model. It's pretty much as usual, except that right after running the `training_op`, we run the `clip_weights` and `clip_weights2` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9574\n",
      "1 Validation accuracy: 0.9702\n",
      "2 Validation accuracy: 0.9696\n",
      "3 Validation accuracy: 0.9772\n",
      "4 Validation accuracy: 0.9778\n",
      "5 Validation accuracy: 0.9774\n",
      "6 Validation accuracy: 0.9816\n",
      "7 Validation accuracy: 0.9808\n",
      "8 Validation accuracy: 0.9816\n",
      "9 Validation accuracy: 0.9814\n",
      "10 Validation accuracy: 0.9822\n",
      "11 Validation accuracy: 0.9838\n",
      "12 Validation accuracy: 0.9832\n",
      "13 Validation accuracy: 0.984\n",
      "14 Validation accuracy: 0.9842\n",
      "15 Validation accuracy: 0.9848\n",
      "16 Validation accuracy: 0.9842\n",
      "17 Validation accuracy: 0.9846\n",
      "18 Validation accuracy: 0.9842\n",
      "19 Validation accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)                 # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a `max_norm_regularizer()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is as usual, except you must run the weights clipping operations after each training operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9558\n",
      "1 Validation accuracy: 0.97\n",
      "2 Validation accuracy: 0.9726\n",
      "3 Validation accuracy: 0.975\n",
      "4 Validation accuracy: 0.976\n",
      "5 Validation accuracy: 0.9784\n",
      "6 Validation accuracy: 0.9814\n",
      "7 Validation accuracy: 0.982\n",
      "8 Validation accuracy: 0.9812\n",
      "9 Validation accuracy: 0.9814\n",
      "10 Validation accuracy: 0.9824\n",
      "11 Validation accuracy: 0.9812\n",
      "12 Validation accuracy: 0.9806\n",
      "13 Validation accuracy: 0.9814\n",
      "14 Validation accuracy: 0.981\n",
      "15 Validation accuracy: 0.9832\n",
      "16 Validation accuracy: 0.9808\n",
      "17 Validation accuracy: 0.982\n",
      "18 Validation accuracy: 0.9816\n",
      "19 Validation accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow-legacy)",
   "language": "python",
   "name": "tensorflow-legacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
